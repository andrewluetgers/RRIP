#!/usr/bin/env python3
"""Compute visual quality metrics for Rust encoder runs.

Reads original and reconstructed tile PNGs from compress/ and decompress/
directories (generated by `origami encode --debug-images`) and computes
PSNR, SSIM, MSE, VIF, Delta E, LPIPS. Updates the manifest.json in-place.

Usage:
    uv run python evals/scripts/compute_metrics.py [run_dirs...]

If no run_dirs given, processes all rs_* runs in evals/runs/.
"""

import argparse
import json
import pathlib
import sys
import re

import numpy as np
from PIL import Image
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim
from skimage.color import rgb2lab, deltaE_cie76
from scipy import ndimage

try:
    from sewar.full_ref import vifp
    HAS_VIF = True
except ImportError:
    HAS_VIF = False

try:
    import torch
    import lpips as lpips_lib
    _lpips_net = None
    HAS_LPIPS = True
except ImportError:
    HAS_LPIPS = False


def calculate_mse(a, b):
    return float(np.mean((a.astype(np.float32) - b.astype(np.float32)) ** 2))


def calculate_vif(img1, img2):
    if not HAS_VIF:
        return None
    try:
        if img1.ndim == 2:
            img1 = np.stack([img1]*3, axis=-1)
            img2 = np.stack([img2]*3, axis=-1)
        return float(vifp(img1, img2))
    except Exception:
        return None


def calculate_delta_e(img1, img2):
    try:
        lab1 = rgb2lab(img1.astype(np.float64) / 255.0)
        lab2 = rgb2lab(img2.astype(np.float64) / 255.0)
        return float(np.mean(deltaE_cie76(lab1, lab2)))
    except Exception:
        return None


def calculate_lpips(img1, img2):
    if not HAS_LPIPS:
        return None
    global _lpips_net
    try:
        if _lpips_net is None:
            _lpips_net = lpips_lib.LPIPS(net='alex')
        t1 = torch.from_numpy(img1.astype(np.float32) / 255.0).permute(2, 0, 1).unsqueeze(0) * 2 - 1
        t2 = torch.from_numpy(img2.astype(np.float32) / 255.0).permute(2, 0, 1).unsqueeze(0) * 2 - 1
        with torch.no_grad():
            return float(_lpips_net(t1, t2).item())
    except Exception:
        return None


def _compute_ms_ssim(img1, img2, weights=None):
    """Compute Multi-Scale SSIM between two grayscale images.

    Uses 5 scales with standard weights. Returns None if image is too small
    (min dimension < 176 needed for 5 downsample levels).
    """
    if weights is None:
        weights = np.array([0.0448, 0.2856, 0.3001, 0.2363, 0.1333])
    n_scales = len(weights)

    # Need at least 2^(n_scales-1) * 11 = 176 pixels in each dimension
    if min(img1.shape[0], img1.shape[1]) < 176:
        return None

    mssim_vals = []
    mcs_vals = []

    for i in range(n_scales):
        s_val = ssim(img1, img2, data_range=255, full=True)
        if isinstance(s_val, tuple):
            mean_ssim = s_val[0]
            ssim_map = s_val[1]
        else:
            mean_ssim = float(s_val)
            ssim_map = None

        mssim_vals.append(mean_ssim)

        # Compute contrast-structure component: cs = (2*sigma12 + C2) / (sigma1^2 + sigma2^2 + C2)
        # For simplicity, use SSIM / luminance approximation
        # At the final scale we use the full SSIM; at intermediate scales we use CS
        if i < n_scales - 1:
            # CS component approximation via SSIM with luminance factored out
            C1 = (0.01 * 255) ** 2
            mu1 = ndimage.uniform_filter(img1.astype(np.float64), size=11)
            mu2 = ndimage.uniform_filter(img2.astype(np.float64), size=11)
            mu1_sq = mu1 ** 2
            mu2_sq = mu2 ** 2
            mu1_mu2 = mu1 * mu2
            sigma1_sq = ndimage.uniform_filter(img1.astype(np.float64) ** 2, size=11) - mu1_sq
            sigma2_sq = ndimage.uniform_filter(img2.astype(np.float64) ** 2, size=11) - mu2_sq
            sigma12 = ndimage.uniform_filter(img1.astype(np.float64) * img2.astype(np.float64), size=11) - mu1_mu2

            C2 = (0.03 * 255) ** 2
            cs_map = (2.0 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)
            mcs_vals.append(float(np.mean(cs_map)))

            # Downsample by 2x
            h, w = img1.shape
            img1 = (img1[0:h-h%2, 0:w-w%2].reshape(h//2, 2, w//2, 2).mean(axis=(1, 3))).astype(np.float32)
            img2 = (img2[0:h-h%2, 0:w-w%2].reshape(h//2, 2, w//2, 2).mean(axis=(1, 3))).astype(np.float32)

    # Combine: product of CS^weight for scales 1..N-1, times SSIM^weight for scale N
    result = 1.0
    for i in range(n_scales - 1):
        result *= max(mcs_vals[i], 1e-10) ** weights[i]
    result *= max(mssim_vals[-1], 1e-10) ** weights[-1]

    return float(np.clip(result, 0.0, 1.0))


def calculate_blockiness(img):
    """Average Sobel gradient magnitude at 8x8 JPEG block boundaries."""
    if img.ndim == 3:
        gray = 0.299 * img[:, :, 0].astype(np.float32) + \
               0.587 * img[:, :, 1].astype(np.float32) + \
               0.114 * img[:, :, 2].astype(np.float32)
    else:
        gray = img.astype(np.float32)

    sx = ndimage.sobel(gray, axis=1)
    sy = ndimage.sobel(gray, axis=0)
    grad = np.sqrt(sx ** 2 + sy ** 2)

    boundary_grads = []
    for x in range(8, gray.shape[1], 8):
        boundary_grads.append(np.mean(grad[:, max(0, x-1):x+2]))
    for y in range(8, gray.shape[0], 8):
        boundary_grads.append(np.mean(grad[max(0, y-1):y+2, :]))

    return float(np.mean(boundary_grads)) if boundary_grads else 0.0


def find_file(directory, pattern):
    """Find a file matching pattern (with any numeric prefix)."""
    for f in sorted(directory.iterdir()):
        if re.search(pattern, f.name):
            return f
    return None


def compute_tile_metrics(original_rgb, reconstructed_rgb):
    """Compute all metrics between original and reconstructed RGB tiles."""
    metrics = {}

    # PSNR on luminance
    y_orig = 0.299 * original_rgb[:,:,0].astype(np.float32) + \
             0.587 * original_rgb[:,:,1].astype(np.float32) + \
             0.114 * original_rgb[:,:,2].astype(np.float32)
    y_recon = 0.299 * reconstructed_rgb[:,:,0].astype(np.float32) + \
              0.587 * reconstructed_rgb[:,:,1].astype(np.float32) + \
              0.114 * reconstructed_rgb[:,:,2].astype(np.float32)

    metrics["final_psnr"] = float(psnr(y_orig, y_recon, data_range=255))
    metrics["final_ssim"] = float(ssim(y_orig, y_recon, data_range=255))
    metrics["final_mse"] = calculate_mse(original_rgb, reconstructed_rgb)

    # MS-SSIM (multi-scale structural similarity)
    ms_ssim_val = _compute_ms_ssim(y_orig, y_recon)
    if ms_ssim_val is not None:
        metrics["final_ms_ssim"] = ms_ssim_val

    vif_val = calculate_vif(original_rgb, reconstructed_rgb)
    if vif_val is not None:
        metrics["final_vif"] = vif_val

    delta_e = calculate_delta_e(original_rgb, reconstructed_rgb)
    if delta_e is not None:
        metrics["final_delta_e"] = delta_e

    lpips_val = calculate_lpips(original_rgb, reconstructed_rgb)
    if lpips_val is not None:
        metrics["final_lpips"] = lpips_val

    # Blockiness (JPEG artifact detection)
    metrics["blockiness"] = calculate_blockiness(reconstructed_rgb)
    metrics["blockiness_delta"] = metrics["blockiness"] - calculate_blockiness(original_rgb)

    return metrics


def process_run(run_dir):
    """Process a single Rust encoder run, computing metrics from debug images."""
    run_dir = pathlib.Path(run_dir)
    manifest_path = run_dir / "manifest.json"
    compress_dir = run_dir / "compress"
    decompress_dir = run_dir / "decompress"

    if not manifest_path.exists():
        print(f"  Skipping {run_dir.name}: no manifest.json")
        return
    if not compress_dir.exists() or not decompress_dir.exists():
        print(f"  Skipping {run_dir.name}: no compress/ or decompress/ dirs")
        return

    with open(manifest_path) as f:
        manifest = json.load(f)

    # Only process Rust manifests (have tiles[] array)
    if not isinstance(manifest.get("tiles"), list):
        print(f"  Skipping {run_dir.name}: not a Rust manifest")
        return

    print(f"  Processing {run_dir.name}...")

    # Build decompression_phase structure matching Python format
    decompression = {"L1": {}, "L0": {}}

    for tile_info in manifest["tiles"]:
        level = tile_info["level"]
        tx = tile_info["tx"]
        ty = tile_info["ty"]
        tile_key = f"tile_{tx}_{ty}"

        # Find original and reconstructed images
        orig_pattern = f"_{level}_{tx}_{ty}_original\\.png$"
        recon_pattern = f"_{level}_{tx}_{ty}_reconstructed\\.png$"

        orig_file = find_file(compress_dir, orig_pattern)
        recon_file = find_file(decompress_dir, recon_pattern)

        if orig_file is None or recon_file is None:
            print(f"    Missing images for {level} ({tx},{ty})")
            continue

        original = np.array(Image.open(orig_file).convert("RGB"))
        reconstructed = np.array(Image.open(recon_file).convert("RGB"))

        metrics = compute_tile_metrics(original, reconstructed)
        decompression[level][tile_key] = metrics

    # Add decompression_phase to manifest
    manifest["decompression_phase"] = decompression

    # Also add size_comparison for viewer compatibility
    l2_bytes = manifest.get("l2_bytes", 0)
    # V2 pipeline: fused_l0_bytes (single residual), no per-tile residual_bytes
    fused_l0 = manifest.get("fused_l0_bytes", 0)
    if fused_l0 > 0:
        # V2 fused pipeline
        manifest["size_comparison"] = {
            "origami_total": manifest.get("total_bytes", l2_bytes + fused_l0),
            "origami_L2_baseline": l2_bytes,
            "origami_L0_residuals": fused_l0,
        }
    else:
        # V1 per-tile pipeline
        l1_res = sum(t.get("residual_bytes", 0) for t in manifest["tiles"] if t["level"] == "L1")
        l0_res = sum(t.get("residual_bytes", 0) for t in manifest["tiles"] if t["level"] == "L0")
        manifest["size_comparison"] = {
            "origami_total": l2_bytes + l1_res + l0_res,
            "origami_L2_baseline": l2_bytes,
            "origami_L1_residuals": l1_res,
            "origami_L0_residuals": l0_res,
        }

    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=2)

    # Print summary
    all_psnr = []
    all_ssim = []
    for level in ["L1", "L0"]:
        for tk, td in decompression[level].items():
            if "final_psnr" in td:
                all_psnr.append(td["final_psnr"])
            if "final_ssim" in td:
                all_ssim.append(td["final_ssim"])

    avg_psnr = np.mean(all_psnr) if all_psnr else 0
    avg_ssim = np.mean(all_ssim) if all_ssim else 0
    print(f"    {len(all_psnr)} tiles: avg PSNR={avg_psnr:.2f} dB, avg SSIM={avg_ssim:.4f}")


def main():
    parser = argparse.ArgumentParser(description="Compute visual metrics for Rust encoder runs")
    parser.add_argument("runs", nargs="*", help="Run directories to process (default: all rs_* in evals/runs/)")
    args = parser.parse_args()

    if args.runs:
        run_dirs = [pathlib.Path(r) for r in args.runs]
    else:
        runs_dir = pathlib.Path("evals/runs")
        run_dirs = sorted(d for d in runs_dir.iterdir()
                         if d.is_dir() and d.name.startswith("rs_"))

    if not run_dirs:
        print("No runs found to process.")
        return

    print(f"Computing metrics for {len(run_dirs)} run(s)...")
    if not HAS_VIF:
        print("  (VIF unavailable — install sewar)")
    if not HAS_LPIPS:
        print("  (LPIPS unavailable — install lpips + torch)")

    for run_dir in run_dirs:
        process_run(run_dir)

    print("\nDone.")


if __name__ == "__main__":
    main()
