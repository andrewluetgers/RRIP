Below is a detailed PRD you can hand to a coding agent to build a **Rust tile server** that serves a standard Deep Zoom pyramid while **dynamically reconstructing L0/L1 tiles** from **L2 + residuals**, with a **RocksDB cache** and a simple **OpenSeadragon webpage.

---

# PRD: Residual-Pyramid Tile Server (Rust + RocksDB + Deep Zoom Viewer)

## 1) Summary

Build a high-performance Rust HTTP server that serves a Deep Zoom Image (DZI) pyramid for a whole-slide image (WSI). Levels **L2 and above** are served directly from the baseline pyramid tiles on disk. Levels **L1 and L0** are **generated on demand** using:

* the **covering L2 tile** (prediction prior)
* **luma-only residual tiles** stored as grayscale JPEG (“L” image)

When any tile under an L2 region is requested, the server must generate and cache the **entire L2-family**:

* 4 tiles at L1
* 16 tiles at L0
  (and optionally the L2 tile itself if requested)

Generated tiles are cached:

* **Hot**: in-memory LRU of encoded JPEG bytes
* **Warm**: persistent cache in RocksDB (encoded JPEG bytes), keyed by `(slide_id, level, x, y)`

The project also serves a simple HTML viewer using OpenSeadragon.

---

## 2) Goals

### Functional goals

1. Serve DZI manifests and tiles compatible with OpenSeadragon.
2. Serve baseline tiles for **levels ≤ L2** as static bytes from disk.
3. For requested tiles in **L1 or L0**, reconstruct tiles using:

    * decode L2
    * upsample prediction
    * add decoded residual (luma only)
    * reuse prediction chroma (Cb/Cr)
    * encode requested tile as JPEG (and generate siblings)
4. On *any* request for a tile covered by an L2 tile, generate and cache **all 20 descendants** (4 + 16).
5. Use RocksDB as a persistent cache so the server does not regenerate tiles repeatedly.
6. Provide a basic OpenSeadragon webpage for interactive pan/zoom.

### Performance goals

* **Hot cache hit (RAM)**: P95 < 10 ms per tile response (excluding network).
* **Warm cache hit (RocksDB)**: P95 < 25 ms per tile response.
* **Cold miss (generate L2-family)**: P95 < 200–500 ms depending on CPU and storage; the target is “fast enough that zoom feels instant after first tile”.
* No “cache stampede”: concurrent requests for siblings under the same L2 should not trigger duplicate reconstruction.

### Operational goals

* Simple deployment (single binary + assets), with config via file/env.
* Metrics + tracing logs for debugging (tile latency, hit rates, generation time).

---

## 3) Non-goals (initial version)

* Not an archival format (this is a serving cache layer).
* Not implementing WISE directly in the tile server (future).
* Not GPU dependence; CPU-only reconstruction.
* No multi-node distributed cache coherence (single node is the target; multiple nodes can run independently).
* No full authentication/authorization (optional later).

---

## 4) Inputs & Outputs

### Inputs (from preprocessing CLI)

Each slide is represented by:

**A) Baseline Deep Zoom pyramid (standard interpolation):**

* `{slide_root}/baseline_pyramid.dzi`
* `{slide_root}/baseline_pyramid_files/{level}/{x}_{y}.jpg`

Deep Zoom levels are numbered **0..N**, where **0 is smallest** and **N is highest resolution**.

**B) Residuals for L0 and L1** (luma-only residual JPEGs) generated by the CLI:

* `{slide_root}/residuals_q{Q}/L1/{x2}_{y2}/{x1}_{y1}.jpg`
* `{slide_root}/residuals_q{Q}/L0/{x2}_{y2}/{x0}_{y0}.jpg`

Where `(x2,y2)` is the covering tile at L2, and `(x1,y1)` or `(x0,y0)` are the child coordinates at L1/L0.

> NOTE: The server must support this file layout exactly.
> Optional optimization later: packfile per L2 parent (see §10).

### Outputs (served over HTTP)

* DZI manifest: `GET /dzi/{slide_id}.dzi`
* Tiles: `GET /tiles/{slide_id}/{level}/{x}_{y}.jpg`
* Viewer: `GET /viewer/{slide_id}` (HTML + JS)

---

## 5) Reconstruction Algorithm (MVP)

### Level mapping

Let Deep Zoom max level be `N = max_level`.

Define:

* `L0 = N` (highest-res)
* `L1 = N-1`
* `L2 = N-2`

For a requested tile `(level, x, y)`:

* if `level <= L2`: serve baseline tile bytes from disk
* if `level == L1`: parent `(x2,y2) = (x>>1, y>>1)`
* if `level == L0`: parent `(x2,y2) = (x>>2, y>>2)`

### Prediction + residual application

1. Decode baseline L2 JPEG tile → obtain RGB or YCbCr (implementation-specific).
2. Upsample L2 to create L1 prediction mosaic (2× linear scale):

    * L2 tile (256×256) → 512×512 mosaic
    * split into 4 predicted L1 tiles
3. For each of 4 L1 tiles:

    * decode residual grayscale JPEG `R` (0..255)
    * convert to signed residual: `r = R - 128`
    * compute recon luma: `Y_recon = clamp(Y_pred + r, 0..255)`
    * use predicted chroma channels for Cb/Cr (no residual applied)
    * encode reconstructed tile to JPEG bytes
4. Build reconstructed L1 mosaic from the 4 reconstructed tiles.
5. Upsample reconstructed L1 mosaic to create L0 prediction mosaic (2× linear scale):

    * 512×512 → 1024×1024
    * split into 16 predicted L0 tiles
6. For each of 16 L0 tiles:

    * decode residual; apply to luma; reuse predicted chroma
    * encode reconstructed tile to JPEG bytes

### Encoding parameters

* Output tile JPEG quality `tile_out_q`: configurable (default 90)
* Output subsampling: configurable

    * default: 4:2:0 (better size + fast)
    * allow 4:4:4 mode if needed

### Codec choice

For performance, use libjpeg-turbo (decode and encode). It should support:

* fast grayscale decode for residuals
* fast JPEG encode for outputs
* optional YCbCr (YUV) decode path for baseline L2

---

## 6) Caching Behavior

### Cache tiers

**Tier 1: RAM LRU (hot)**

* Stores: encoded JPEG bytes for tiles
* Key: `(slide_id, level, x, y)`
* Size cap: configurable (e.g., 2–8 GB depending on RAM)

**Tier 2: RocksDB (warm)**

* Stores: encoded JPEG bytes for tiles
* Key: `tile:{slide_id}:{level}:{x}:{y}`
* Value: raw JPEG bytes (no extra compression)
* Writes: done via WriteBatch when generating family
* Reads: single `Get()` per requested tile

### Generation unit = L2 family

On a cache miss for any L0/L1 tile, the server:

1. Computes `(x2,y2)` parent
2. Acquires a singleflight “in progress” lock for that family
3. Generates all 20 tiles (4 L1 + 16 L0)
4. Inserts all 20 tiles:

    * RAM LRU (immediately)
    * RocksDB (WriteBatch)
5. Releases lock
6. Serves requested tile bytes

### Singleflight / stampede control

* A concurrent request for any sibling tile under the same `(slide_id, x2, y2)` should:

    * wait for the same generation future to complete, then serve from hot cache
* Timeouts: configurable; if generation fails, fall back to serving baseline where possible, or return 500.

### Optional: prewarm

If a request is for L2 tile itself:

* Serve L2 immediately (baseline bytes)
* Trigger async family generation for `(x2,y2)` in background (non-blocking)

---

## 7) HTTP API

### Tile + manifest endpoints

1. `GET /dzi/{slide_id}.dzi`

    * returns baseline `baseline_pyramid.dzi` (may need URL rewrite in manifest; see below)

2. `GET /tiles/{slide_id}/{level}/{x}_{y}.jpg`

    * if `level <= L2`: read from baseline pyramid folder
    * if `level == L1 or L0`: serve from cache or generate family

3. `GET /viewer/{slide_id}`

    * serves HTML that loads OpenSeadragon using the above DZI

4. `GET /healthz`

5. `GET /metrics` (Prometheus format recommended)

### DZI URL rewriting

OpenSeadragon expects tile URLs based on the DZI’s `Url` field.
You can:

* Serve the DZI with `Url="/tiles/{slide_id}/"` and implement the tile path as `/tiles/{slide_id}/{level}/{x}_{y}.jpg`
* Or keep DZI unmodified and mount routes to match `{prefix}_files/...` semantics

**Requirement:** must work in a browser with OpenSeadragon with zero manual edits.

---

## 8) Filesystem Layout & Slide Registration

### Slide root structure (per slide_id)

```
slides/{slide_id}/
  baseline_pyramid.dzi
  baseline_pyramid_files/
    {level}/
      {x}_{y}.jpg
  residuals_q32/
    L1/{x2}_{y2}/{x1}_{y1}.jpg
    L0/{x2}_{y2}/{x0}_{y0}.jpg
```

### Slide registry

Server discovers slides via a config-defined `slides_root/`.

* `slide_id` is the directory name.
* On startup, scan slide roots and compute:

    * max DZI level N
    * derived L0/L1/L2 numbers
    * tile size (from DZI) for validation

---

## 9) Observability

### Metrics

* `tile_requests_total{level,cache="ram|rocks|miss|baseline"}`
* `tile_request_latency_ms_bucket{level,cache}`
* `family_generation_total`
* `family_generation_latency_ms_bucket`
* `rocksdb_get_latency_ms_bucket`
* `rocksdb_put_bytes_total`
* `ram_cache_bytes`
* `ram_cache_hit_rate`

### Logs

Structured logs (JSON) including:

* request id
* slide_id, level, x, y
* cache status
* parent x2,y2
* generation timing breakdown (decode L2, upsample, residual decode, encode, db write)

---

## 10) Performance Options (Design for MVP vs “fastest possible”)

### MVP (simpler, still fast)

* Decode baseline L2 to RGB
* Convert to YCbCr in Rust
* Apply residual to Y
* Encode JPEG from RGB (or from YCbCr if encoder supports)

### High-performance path (recommended target)

* Decode baseline L2 directly to YCbCr/YUV using libjpeg-turbo
* Keep all processing in luma plane + predicted chroma planes
* Encode output tiles from YCbCr directly
* Custom “2× bilinear” scaler (fast integer math)

### Residual storage optimization (recommended soon)

Add an optional preprocessing step to pack residuals per L2 parent to avoid 20 small file reads:

```
residual_packs/
  L2/{x2}_{y2}.pack   # contains 4+16 residual JPEG streams + index
```

Server should support:

* if pack exists: 1 file read per family
* else: fallback to existing residual directory

(Agent can implement pack format after MVP tile-serving correctness is proven.)

---

## 11) RocksDB Schema & Settings

### Keys

* `tile:{slide_id}:{level}:{x}:{y}` → JPEG bytes

Optional:

* `meta:{slide_id}:N` → max_level
* `meta:{slide_id}:tile_size` → int
* `meta:{slide_id}:created_at` → timestamp

### Write policy

* Use `WriteBatch` for the 20 family tiles
* `disableWAL=true` configurable (cache can be rebuilt; boosts throughput)

### Eviction policy (choose one for v1)

Because RocksDB isn’t an LRU cache by default, define one of:

1. **TTL via compaction filter** (preferred)

    * store `created_at` in value header
    * drop entries older than `cache_ttl_days`
2. **Manual slide cache reset**

    * store cache per slide in separate DB directory and delete whole DB for cleanup
3. **No eviction (dev mode)**

PRD requirement: implement at least (2) or (3) in v1; implement TTL in v2 if needed.

---

## 12) Viewer Requirements

Serve:

* `/viewer/{slide_id}` HTML page
* includes OpenSeadragon JS (local vendored file or CDN; prefer vendored for offline)
* loads tile source from `/dzi/{slide_id}.dzi`
* basic UI:

    * zoom controls
    * current level display (optional)
    * debug overlay (optional): tile coords under cursor

---

## 13) Implementation Requirements (Rust)

### Recommended stack

* HTTP server: Axum (tokio + hyper) or Actix-web (either acceptable)
* RocksDB crate: `rocksdb`
* JPEG: turbojpeg bindings (agent selects a stable crate)
* LRU cache: `lru` crate or custom clock-pro cache

### Concurrency

* Use bounded tokio runtime worker threads (configurable)
* Per-family singleflight map:

    * key: `(slide_id, x2, y2)`
    * value: `JoinHandle` or shared future

### Memory management

* Cache stores encoded bytes as `Bytes` (zero-copy where possible)
* Avoid frequent allocations during mosaic upsample and residual apply:

    * reuse buffers in a per-worker pool if possible

---

## 14) Validation, Quality, and Correctness

### Correctness tests

* Coordinate mapping tests: ensure `(level,x,y)` maps to correct parent `(x2,y2)`
* Reconstruction tests against Python reference for a small sample slide:

    * generate tile in Rust and compare to Python output (byte-level may differ due to encoder, so compare PSNR on luma + ΔE00)
* Cache tests:

    * first request miss → generates family → next sibling request is hot hit
    * warm hit from RocksDB after RAM is cleared

### Quality gates (configurable)

For sampled tiles:

* PSNR(Y) >= threshold (e.g., 35 dB) at residual Q=32 baseline
* Mean ΔE00 <= threshold (e.g., 2–5) depending on policy

(These are sanity checks; pathology acceptance is a separate validation.)

---

## 15) Acceptance Criteria

1. Given a slide directory with `baseline_pyramid.*` and `residuals_q32`, the server:

    * serves `/viewer/{slide_id}` and the image pans/zooms correctly.
2. Requests for L0/L1 tiles:

    * trigger exactly **one** family generation per L2 region under concurrency
    * cache all 20 tiles (4 L1, 16 L0) in RAM + RocksDB
3. Compression/caching stats:

    * logs show bytes generated, cache hit rates, generation times
4. Restart behavior:

    * after restart, warm cache in RocksDB is reused (no regeneration needed for previously generated tiles)
5. Performance:

    * hot cache hits are noticeably faster than warm hits; warm hits faster than cold misses

---

## 16) Milestones

### M1 — MVP correctness

* Static serving of L2+ from baseline pyramid
* Dynamic L0/L1 reconstruction (no RocksDB yet), RAM cache only
* Viewer page works

### M2 — RocksDB warm cache

* WriteBatch insertion for family tiles
* Read-through caching
* basic metrics and logs

### M3 — Singleflight + neighbor generation

* stampede control implemented
* family generation on any descendant request
* async prewarm on L2 requests

### M4 — Residual packfile optimization (optional but recommended)

* pack residuals per L2 parent
* server prefers pack reads over 20 individual reads

---

## 17) Open Questions / Config Defaults (agent can hardcode in v1)

* Tile size: default 256
* Output JPEG subsampling: default 4:2:0
* Output JPEG quality: default 90
* Residual quality implied by folder name (q32); server config chooses which residual folder to use
* Cache TTL: optional in v1

---

If you want, I can also add a **“pack residuals per L2 parent”** spec (exact binary format + index layout) and a **RocksDB eviction design** that’s realistic at multi-terabyte scale—those two details tend to matter a lot once you deploy beyond a single demo slide.
