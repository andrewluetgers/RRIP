{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea36ec91",
   "metadata": {},
   "source": [
    "# WSI Residual Pyramid Tool (pyvips + JPEG-L residuals)\n",
    "\n",
    "This notebook builds a **standard interpolated quadtree tile pyramid** (Deep Zoom layout) using **pyvips**,\n",
    "then replaces the two highest-resolution levels (**L0 and L1**) with **luma-only residual JPEGs**\n",
    "(grayscale JPEG, quality configurable; default **Q=32**) conditioned on the **covering L2 tile**.\n",
    "\n",
    "Outputs:\n",
    "- Baseline Deep Zoom pyramid tiles (JPEG Q=90 by default)\n",
    "- Residual JPEG-L tiles for L1 and L0\n",
    "- Storage stats and compression ratios\n",
    "- Optional quality stats on a random sample: **PSNR on luma** and **ΔE00**\n",
    "\n",
    "> For large slides, this is I/O heavy. Start with a limited number of L2 parents (`MAX_PARENTS`) to validate."
   ]
  },
  {
   "cell_type": "code",
   "id": "854c0cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T18:46:29.410290Z",
     "start_time": "2026-01-31T18:46:29.235687Z"
    }
   },
   "source": [
    "# Install notes (run in your environment)\n",
    "# System packages (Ubuntu example):\n",
    "#   sudo apt-get update\n",
    "#   sudo apt-get install -y libvips libvips-dev openslide-tools libopenslide0\n",
    "#\n",
    "# Python packages:\n",
    "#   pip install pyvips Pillow numpy scikit-image openslide-python requests tqdm\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.2 (main, Feb 12 2025, 14:59:08) [Clang 19.1.6 ]\n",
      "Platform: macOS-15.6.1-arm64-arm-64bit-Mach-O\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e573e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: download OpenSlide DICOM testdata\n",
    "import pathlib, zipfile, requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "TESTDATA_URL = \"https://openslide.cs.cmu.edu/download/openslide-testdata/DICOM/3DHISTECH-1.zip\"\n",
    "DATA_DIR = pathlib.Path(\"data\"); DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "ZIP_PATH = DATA_DIR / \"3DHISTECH-1.zip\"\n",
    "EXTRACT_DIR = DATA_DIR\n",
    "\n",
    "def download(url: str, dst: pathlib.Path, chunk=1024*1024):\n",
    "    r = requests.get(url, stream=True, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    total = int(r.headers.get(\"Content-Length\", 0))\n",
    "    tmp = dst.with_suffix(dst.suffix + \".part\")\n",
    "    with open(tmp, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=f\"Downloading {dst.name}\") as pbar:\n",
    "        for b in r.iter_content(chunk_size=chunk):\n",
    "            if b:\n",
    "                f.write(b)\n",
    "                pbar.update(len(b))\n",
    "    tmp.rename(dst)\n",
    "\n",
    "if not ZIP_PATH.exists():\n",
    "    download(TESTDATA_URL, ZIP_PATH)\n",
    "else:\n",
    "    print(\"Zip already present:\", ZIP_PATH)\n",
    "\n",
    "if not EXTRACT_DIR.exists():\n",
    "    print(\"Extracting...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    print(\"Extracted to:\", EXTRACT_DIR)\n",
    "else:\n",
    "    print(\"Already extracted:\", EXTRACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point this at your slide file (SVS/TIFF/MRXS/DICOM/etc).\n",
    "# For the testdata, we just pick a large DICOM file as a starting point.\n",
    "import pathlib\n",
    "\n",
    "ROOT = pathlib.Path(\"data\")\n",
    "if not ROOT.exists():\n",
    "    raise FileNotFoundError(f\"{ROOT} not found. Run the download cell or set ROOT to your dataset.\")\n",
    "\n",
    "cands = sorted(ROOT.rglob(\"*.dcm\"), key=lambda p: p.stat().st_size, reverse=True)\n",
    "print(\"DICOM candidates:\", len(cands))\n",
    "SLIDE_PATH = str(cands[0]) if cands else None\n",
    "print(\"Selected:\", SLIDE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load slide with pyvips (preferred). Requires libvips built with OpenSlide support.\n",
    "def import_pyvips():\n",
    "    import pyvips\n",
    "    return pyvips\n",
    "\n",
    "pyvips = None\n",
    "try:\n",
    "    pyvips = import_pyvips()\n",
    "    print(\"pyvips:\", pyvips.__version__)\n",
    "except Exception as e:\n",
    "    print(\"pyvips import failed:\", e)\n",
    "\n",
    "vips_slide = None\n",
    "if pyvips is not None and SLIDE_PATH:\n",
    "    try:\n",
    "        if hasattr(pyvips.Image, \"openslideload\"):\n",
    "            vips_slide = pyvips.Image.openslideload(SLIDE_PATH)\n",
    "        else:\n",
    "            vips_slide = pyvips.Image.new_from_file(SLIDE_PATH, access=\"sequential\")\n",
    "        print(\"Loaded slide via pyvips:\", vips_slide.width, vips_slide.height, \"bands:\", vips_slide.bands)\n",
    "    except Exception as e:\n",
    "        print(\"pyvips couldn't load this path; you may need to point SLIDE_PATH at the correct WSI entry file.\")\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "if vips_slide is None:\n",
    "    raise RuntimeError(\"Could not load slide with pyvips. Ensure libvips has OpenSlide support, or change SLIDE_PATH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a standard Deep Zoom pyramid (baseline)\n",
    "import pathlib, shutil\n",
    "\n",
    "OUT_DIR = pathlib.Path(\"out\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "PFX = OUT_DIR / \"baseline_pyramid\"   # produces baseline_pyramid.dzi + baseline_pyramid_files/\n",
    "TILE_SIZE = 256\n",
    "BASE_JPEG_Q = 90\n",
    "\n",
    "# Clean old outputs\n",
    "for p in [PFX.with_suffix(\".dzi\"), OUT_DIR / (PFX.name + \"_files\")]:\n",
    "    if p.exists():\n",
    "        if p.is_dir(): shutil.rmtree(p)\n",
    "        else: p.unlink()\n",
    "\n",
    "vips_slide.dzsave(\n",
    "    str(PFX),\n",
    "    tile_size=TILE_SIZE,\n",
    "    overlap=0,\n",
    "    suffix=f\".jpg[Q={BASE_JPEG_Q}]\",\n",
    "    depth=\"one\",\n",
    "    layout=\"dz\",\n",
    ")\n",
    "print(\"Wrote:\", PFX.with_suffix(\".dzi\"))\n",
    "print(\"Tiles at:\", OUT_DIR / (PFX.name + \"_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode residuals for the two highest-res levels (L0/L1) conditioned on L2.\n",
    "import os, re, json, math, random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import color as skcolor\n",
    "\n",
    "BASE_FILES = OUT_DIR / (PFX.name + \"_files\")\n",
    "levels = sorted([int(p.name) for p in BASE_FILES.iterdir() if p.is_dir()])\n",
    "max_level = max(levels)\n",
    "L0 = max_level\n",
    "L1 = max_level - 1\n",
    "L2 = max_level - 2\n",
    "print(\"DeepZoom levels:\", levels)\n",
    "print(\"Using L0/L1/L2 =\", L0, L1, L2)\n",
    "\n",
    "RES_Q = 32               # residual grayscale JPEG quality\n",
    "UPSAMPLE = Image.Resampling.BILINEAR\n",
    "\n",
    "RESID_DIR = OUT_DIR / f\"residuals_q{RES_Q}\"\n",
    "if RESID_DIR.exists():\n",
    "    import shutil\n",
    "    shutil.rmtree(RESID_DIR)\n",
    "RESID_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def tile_path(level:int, x:int, y:int):\n",
    "    return BASE_FILES / str(level) / f\"{x}_{y}.jpg\"\n",
    "\n",
    "def load_rgb(p):\n",
    "    return np.array(Image.open(p).convert(\"RGB\"))\n",
    "\n",
    "def save_gray_jpeg(arr_u8, p, q):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    Image.fromarray(arr_u8.astype(np.uint8), mode=\"L\").save(p, format=\"JPEG\", quality=int(q), optimize=True)\n",
    "\n",
    "def rgb_to_ycbcr_bt601(rgb_u8):\n",
    "    rgb = rgb_u8.astype(np.float32)\n",
    "    R,G,B = rgb[...,0], rgb[...,1], rgb[...,2]\n",
    "    Y  = 0.299*R + 0.587*G + 0.114*B\n",
    "    Cb = -0.168736*R - 0.331264*G + 0.5*B + 128.0\n",
    "    Cr = 0.5*R - 0.418688*G - 0.081312*B + 128.0\n",
    "    return Y, Cb, Cr\n",
    "\n",
    "def ycbcr_to_rgb_bt601(Y, Cb, Cr):\n",
    "    Y = Y.astype(np.float32)\n",
    "    Cb = Cb.astype(np.float32) - 128.0\n",
    "    Cr = Cr.astype(np.float32) - 128.0\n",
    "    R = Y + 1.402*Cr\n",
    "    G = Y - 0.344136*Cb - 0.714136*Cr\n",
    "    B = Y + 1.772*Cb\n",
    "    return np.clip(np.stack([R,G,B], axis=-1), 0, 255).astype(np.uint8)\n",
    "\n",
    "def psnr(a, b, data_range=255.0):\n",
    "    a=a.astype(np.float32); b=b.astype(np.float32)\n",
    "    mse = np.mean((a-b)**2)\n",
    "    if mse == 0: return float(\"inf\")\n",
    "    return 10*math.log10((data_range**2)/mse)\n",
    "\n",
    "def psnr_luma(gt_rgb, est_rgb):\n",
    "    Y1,_,_ = rgb_to_ycbcr_bt601(gt_rgb)\n",
    "    Y2,_,_ = rgb_to_ycbcr_bt601(est_rgb)\n",
    "    return psnr(Y1, Y2)\n",
    "\n",
    "def deltaE00_mean_p95(rgb1, rgb2):\n",
    "    lab1 = skcolor.rgb2lab(rgb1.astype(np.float32)/255.0)\n",
    "    lab2 = skcolor.rgb2lab(rgb2.astype(np.float32)/255.0)\n",
    "    de = skcolor.deltaE_ciede2000(lab1, lab2).reshape(-1)\n",
    "    return float(np.mean(de)), float(np.percentile(de, 95))\n",
    "\n",
    "# Enumerate L2 tiles\n",
    "l2_tiles=[]\n",
    "for f in (BASE_FILES/str(L2)).glob(\"*.jpg\"):\n",
    "    m=re.match(r\"(\\\\d+)_(\\\\d+)\\\\.jpg$\", f.name)\n",
    "    if m:\n",
    "        l2_tiles.append((int(m.group(1)), int(m.group(2))))\n",
    "print(\"L2 tiles:\", len(l2_tiles))\n",
    "\n",
    "# SAFETY: start with a subset, then set MAX_PARENTS=None for full run\n",
    "MAX_PARENTS = 200   # set to None for full processing\n",
    "if MAX_PARENTS:\n",
    "    random.shuffle(l2_tiles)\n",
    "    l2_tiles = l2_tiles[:MAX_PARENTS]\n",
    "print(\"Processing L2 parents:\", len(l2_tiles))\n",
    "\n",
    "# Storage accounting\n",
    "baseline_bytes = sum(f.stat().st_size for lv in levels for f in (BASE_FILES/str(lv)).glob(\"*.jpg\"))\n",
    "retained_bytes = sum(f.stat().st_size for lv in levels if lv <= L2 for f in (BASE_FILES/str(lv)).glob(\"*.jpg\"))\n",
    "\n",
    "residual_bytes_L1 = 0\n",
    "residual_bytes_L0 = 0\n",
    "\n",
    "# Quality sampling\n",
    "sample_records = []\n",
    "SAMPLE_PROB = 0.01   # raise for more metrics\n",
    "\n",
    "for (x2,y2) in l2_tiles:\n",
    "    p2 = tile_path(L2, x2, y2)\n",
    "    if not p2.exists():\n",
    "        continue\n",
    "    l2 = load_rgb(p2)\n",
    "\n",
    "    # Predict L1 mosaic from L2\n",
    "    l1_mosaic_pred = np.array(Image.fromarray(l2).resize((TILE_SIZE*2, TILE_SIZE*2), resample=UPSAMPLE))\n",
    "\n",
    "    # Reconstruct all 4 L1 tiles\n",
    "    recon_l1_tiles = [[None,None],[None,None]]\n",
    "    for dy in range(2):\n",
    "        for dx in range(2):\n",
    "            x1 = x2*2 + dx\n",
    "            y1 = y2*2 + dy\n",
    "            p1 = tile_path(L1, x1, y1)\n",
    "            if not p1.exists():\n",
    "                continue\n",
    "            gt1 = load_rgb(p1)\n",
    "            pred1 = l1_mosaic_pred[dy*TILE_SIZE:(dy+1)*TILE_SIZE, dx*TILE_SIZE:(dx+1)*TILE_SIZE]\n",
    "\n",
    "            Yg,_,_ = rgb_to_ycbcr_bt601(gt1)\n",
    "            Yp,Cbp,Crp = rgb_to_ycbcr_bt601(pred1)\n",
    "\n",
    "            Ry = Yg - Yp\n",
    "            enc = np.clip(np.round(Ry + 128.0), 0, 255).astype(np.uint8)\n",
    "\n",
    "            rp = RESID_DIR / \"L1\" / f\"{x2}_{y2}\" / f\"{x1}_{y1}.jpg\"\n",
    "            save_gray_jpeg(enc, rp, RES_Q)\n",
    "            residual_bytes_L1 += rp.stat().st_size\n",
    "\n",
    "            # decode residual and reconstruct\n",
    "            r_dec = np.array(Image.open(rp).convert(\"L\")).astype(np.float32) - 128.0\n",
    "            Yhat = np.clip(Yp + r_dec, 0, 255)\n",
    "            recon_l1_tiles[dy][dx] = ycbcr_to_rgb_bt601(Yhat, Cbp, Crp)\n",
    "\n",
    "    if any(recon_l1_tiles[dy][dx] is None for dy in range(2) for dx in range(2)):\n",
    "        continue\n",
    "\n",
    "    recon_l1_mosaic = np.concatenate([np.concatenate(row, axis=1) for row in recon_l1_tiles], axis=0)\n",
    "\n",
    "    # Predict L0 mosaic from reconstructed L1 mosaic\n",
    "    l0_mosaic_pred = np.array(Image.fromarray(recon_l1_mosaic).resize((TILE_SIZE*4, TILE_SIZE*4), resample=UPSAMPLE))\n",
    "\n",
    "    for dy in range(4):\n",
    "        for dx in range(4):\n",
    "            x0 = x2*4 + dx\n",
    "            y0 = y2*4 + dy\n",
    "            p0 = tile_path(L0, x0, y0)\n",
    "            if not p0.exists():\n",
    "                continue\n",
    "            gt0 = load_rgb(p0)\n",
    "            pred0 = l0_mosaic_pred[dy*TILE_SIZE:(dy+1)*TILE_SIZE, dx*TILE_SIZE:(dx+1)*TILE_SIZE]\n",
    "\n",
    "            Yg,_,_ = rgb_to_ycbcr_bt601(gt0)\n",
    "            Yp,Cbp,Crp = rgb_to_ycbcr_bt601(pred0)\n",
    "\n",
    "            Ry = Yg - Yp\n",
    "            enc = np.clip(np.round(Ry + 128.0), 0, 255).astype(np.uint8)\n",
    "\n",
    "            rp = RESID_DIR / \"L0\" / f\"{x2}_{y2}\" / f\"{x0}_{y0}.jpg\"\n",
    "            save_gray_jpeg(enc, rp, RES_Q)\n",
    "            residual_bytes_L0 += rp.stat().st_size\n",
    "\n",
    "            if random.random() < SAMPLE_PROB:\n",
    "                r_dec = np.array(Image.open(rp).convert(\"L\")).astype(np.float32) - 128.0\n",
    "                Yhat = np.clip(Yp + r_dec, 0, 255)\n",
    "                recon0 = ycbcr_to_rgb_bt601(Yhat, Cbp, Crp)\n",
    "\n",
    "                sample_records.append({\n",
    "                    \"level\": \"L0\",\n",
    "                    \"x\": x0, \"y\": y0,\n",
    "                    \"psnrY\": psnr_luma(gt0, recon0),\n",
    "                    \"deltaE_mean\": deltaE00_mean_p95(gt0, recon0)[0],\n",
    "                    \"deltaE_p95\": deltaE00_mean_p95(gt0, recon0)[1],\n",
    "                    \"residual_bytes\": rp.stat().st_size\n",
    "                })\n",
    "\n",
    "proposed_bytes = retained_bytes + residual_bytes_L1 + residual_bytes_L0\n",
    "summary = {\n",
    "    \"tile_size\": TILE_SIZE,\n",
    "    \"base_jpeg_q\": BASE_JPEG_Q,\n",
    "    \"residual_jpeg_q_L\": RES_Q,\n",
    "    \"dz_levels\": {\"L0\": L0, \"L1\": L1, \"L2\": L2},\n",
    "    \"baseline_bytes_all_levels\": baseline_bytes,\n",
    "    \"retained_bytes_L2plus\": retained_bytes,\n",
    "    \"residual_bytes_L1\": residual_bytes_L1,\n",
    "    \"residual_bytes_L0\": residual_bytes_L0,\n",
    "    \"proposed_bytes\": proposed_bytes,\n",
    "    \"compression_ratio\": baseline_bytes / proposed_bytes,\n",
    "    \"savings_pct\": (1 - proposed_bytes / baseline_bytes) * 100.0,\n",
    "    \"metrics_samples\": len(sample_records),\n",
    "}\n",
    "\n",
    "(OUT_DIR / \"summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# If you want CSV of sample_records:\n",
    "# import pandas as pd\n",
    "# pd.DataFrame(sample_records).to_csv(OUT_DIR/\"metrics_samples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read summary\n",
    "import json, pathlib\n",
    "p = pathlib.Path(\"out/summary.json\")\n",
    "print(p.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kasxefdhx6",
   "source": "## BD-Rate and BD-PSNR Analysis\n\nNow we'll analyze the compression efficiency using Bjøntegaard Delta metrics. BD-Rate tells us the average bitrate savings, while BD-PSNR tells us the average quality improvement.\n\n**Interpretation:**\n- **BD-Rate < 0**: Test codec (RRIP) uses less bitrate than reference (standard JPEG) for same quality\n- **BD-PSNR > 0**: Test codec (RRIP) provides better quality than reference for same bitrate",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "51zvooru94q",
   "source": "# Import BD metrics module\nimport sys\nsys.path.append('..')  # Add parent directory to path to import cli modules\nfrom cli.bd_metrics import BDMetrics, RDCurveAnalyzer\n\n# Function to run multiple quality settings and collect RD points\ndef collect_rd_points_for_quality_settings(vips_slide, qualities, residual_qualities, \n                                          sample_tiles=10, tile_level=\"L0\"):\n    \"\"\"\n    Collect rate-distortion points for different quality settings.\n    \n    Args:\n        vips_slide: The loaded slide image\n        qualities: List of JPEG qualities for baseline\n        residual_qualities: List of residual JPEG qualities for RRIP\n        sample_tiles: Number of tiles to sample for metrics\n        tile_level: Which level to measure (\"L0\" or \"L1\")\n    \"\"\"\n    analyzer = RDCurveAnalyzer()\n    \n    # For demonstration, we'll simulate the process\n    # In practice, you'd run the actual compression for each quality setting\n    \n    print(f\"Collecting RD points for {tile_level} tiles...\")\n    print(\"=\" * 60)\n    \n    # Baseline JPEG at different qualities\n    print(\"\\nBaseline JPEG (standard pyramid):\")\n    for q in qualities:\n        # Here you would actually compress with this quality and measure\n        # For now, we'll use approximate relationships\n        bitrate = 1000000 * (q/100)**2  # Simplified model\n        psnr = 30 + 15 * (q/100)  # Simplified model\n        \n        analyzer.add_reference_point(bitrate, psnr, q)\n        print(f\"  Q{q:3d}: {bitrate/1e6:.2f} MB, {psnr:.1f} dB\")\n    \n    # RRIP at different residual qualities\n    print(\"\\nRRIP (residual pyramid):\")\n    for q in residual_qualities:\n        # Actual implementation would run the residual encoding\n        # These are example relationships - replace with actual measurements\n        bitrate = 500000 * (q/64)**1.5  # Residuals compress better\n        psnr = 32 + 10 * (q/64)  # Good quality preservation\n        \n        analyzer.add_test_point(bitrate, psnr, q)\n        print(f\"  Q{q:3d}: {bitrate/1e6:.2f} MB, {psnr:.1f} dB\")\n    \n    return analyzer\n\n# Define quality settings to test\nbaseline_qualities = [95, 90, 85, 80, 75, 70]\nresidual_qualities = [64, 48, 32, 24, 16, 8]\n\n# Collect RD points (using simulated data for demonstration)\nanalyzer = collect_rd_points_for_quality_settings(\n    vips_slide, \n    baseline_qualities, \n    residual_qualities,\n    sample_tiles=10,\n    tile_level=\"L0\"\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "m9f4bvdyog9",
   "source": "# Calculate BD metrics\ntry:\n    metrics = analyzer.calculate_metrics()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"BD METRICS RESULTS:\")\n    print(\"=\" * 60)\n    print(f\"BD-Rate: {metrics['bd_rate']:.2f}%\")\n    if metrics['bd_rate'] < 0:\n        print(f\"  → RRIP uses {abs(metrics['bd_rate']):.1f}% LESS bitrate for same quality ✓\")\n    else:\n        print(f\"  → RRIP uses {metrics['bd_rate']:.1f}% MORE bitrate for same quality\")\n    \n    print(f\"\\nBD-PSNR: {metrics['bd_psnr']:.2f} dB\")\n    if metrics['bd_psnr'] > 0:\n        print(f\"  → RRIP provides {metrics['bd_psnr']:.2f} dB BETTER quality for same bitrate ✓\")\n    else:\n        print(f\"  → RRIP provides {abs(metrics['bd_psnr']):.2f} dB WORSE quality for same bitrate\")\n        \n    print(\"\\nINTERPRETATION:\")\n    if metrics['bd_rate'] < -20:\n        print(\"  • Excellent compression efficiency gain (>20% bitrate reduction)\")\n    elif metrics['bd_rate'] < -10:\n        print(\"  • Good compression efficiency gain (10-20% bitrate reduction)\")\n    elif metrics['bd_rate'] < 0:\n        print(\"  • Modest compression efficiency gain (<10% bitrate reduction)\")\n    else:\n        print(\"  • No compression efficiency gain\")\n        \nexcept Exception as e:\n    print(f\"Error calculating BD metrics: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uh4u2g88van",
   "source": "# Plot Rate-Distortion curves\nimport matplotlib.pyplot as plt\n\nfig = analyzer.plot_rd_curves(\n    title=\"RRIP vs Standard JPEG: Rate-Distortion Performance\",\n    show_bd_metrics=True\n)\nplt.show()\n\n# Save the plot\nfig.savefig(OUT_DIR / \"rd_curves.png\", dpi=300, bbox_inches='tight')\nprint(f\"RD curves saved to: {OUT_DIR / 'rd_curves.png'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lm84fhfxw6",
   "source": "## Production BD Analysis with Actual Measurements\n\nFor production analysis, replace the simulated data above with actual measurements from your compression pipeline. Here's a template for collecting real RD points:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7vcwzfmuu0w",
   "source": "# Template for actual RD point collection\ndef measure_actual_rd_points(slide_path, output_dir, quality_settings):\n    \"\"\"\n    Measure actual rate-distortion points for different quality settings.\n    \n    This function should:\n    1. Run compression at each quality setting\n    2. Measure actual file sizes (bitrate)\n    3. Calculate PSNR by comparing reconstructed vs original\n    4. Return the RD points\n    \"\"\"\n    rd_points = []\n    \n    for quality in quality_settings:\n        # Run your compression pipeline\n        # compressed_size = run_compression(slide_path, output_dir, quality)\n        # psnr_value = calculate_psnr(original, reconstructed)\n        # rd_points.append((quality, compressed_size, psnr_value))\n        pass\n    \n    return rd_points\n\n# Example of how to use with actual data:\n\"\"\"\n# Baseline JPEG measurements\nbaseline_rd_points = measure_actual_rd_points(\n    SLIDE_PATH, \n    OUT_DIR / \"baseline\",\n    qualities=[95, 90, 85, 80, 75, 70, 60, 50]\n)\n\n# RRIP measurements  \nrrip_rd_points = measure_actual_rd_points(\n    SLIDE_PATH,\n    OUT_DIR / \"rrip\", \n    qualities=[64, 48, 32, 24, 16, 12, 8, 4]\n)\n\n# Create analyzer with real data\nreal_analyzer = RDCurveAnalyzer()\n\nfor quality, bitrate, psnr in baseline_rd_points:\n    real_analyzer.add_reference_point(bitrate, psnr, quality)\n    \nfor quality, bitrate, psnr in rrip_rd_points:\n    real_analyzer.add_test_point(bitrate, psnr, quality)\n\n# Calculate BD metrics\nreal_metrics = real_analyzer.calculate_metrics()\nprint(f\"Actual BD-Rate: {real_metrics['bd_rate']:.2f}%\")\nprint(f\"Actual BD-PSNR: {real_metrics['bd_psnr']:.2f} dB\")\n\"\"\"\nprint(\"Template for actual RD measurement created. Uncomment and modify for your pipeline.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}